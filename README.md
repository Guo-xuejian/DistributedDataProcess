### DistributedDataProcessing
## 云函数调用
所有的函数文件都可以上传至lzudatafactory中去，但是我在本地上很难有两台机器，所以选择了不同的端口来完成
lzudatafactory选择7000端口，而分机数据处理选择8000端口
两者需要分别启动，client文件夹中的config.py文件中是如此定义端口的，如有修改请一同完成修改

    lzudatafactory
    python manage.py runserver 7000
    分机数据处理
    python manage.py runserver 8000
    

##分机数据处理

目前支持三种任务
    电力大数据预测，上传格式pkl，运算IP下载数据集文件格式pkl，IP上传结果格式pkl，最终的结果csv文件中为一列数据，行数为上传的数据行数
    大整数分解，上传格式pkl（若干行数据，每一行为一个需要进行分解的大整数），运算IP下载数据集格式pkl，IP上传结果格式pkl，最终结果文件中行数为数据行数，列数为该行对应大整数分解之后的数据总数，比如819	3	3	7	13，第一个数字为大整数，之后对应分解的数据
    文本中特定文字出现次数统计，上传文件格式txt，下载格式txt，上传格式pkl，最终文件为一列数据，行数为定义的函数中设定的所有希望统计的文字，比如测试的时候用的是“刀”，“剑”等等
    
    

这个后台工作逻辑大概如下：
    所有的局域网中的用户都可以访问作为服务端的机器来申请获得一个数据集用来运算并返回相应的结果，django的后台现在几乎是只能查看相应数据，已经把能够操作的按钮删除的差不多了，后台中“分级处理——任务”页面中保留了两个按钮，但是目前推荐是在所有的任务都完成之后再点击，因为两个按钮分别为“文件聚合”和“预测下载”，前者将会将后台项目中submit_files文件夹中对应后台选择任务名字和日期的任务对应的所有运算结果（由所有参与申请运算的机器上传的最终的结果文件，目前考虑的是保留下来方便查错，但是也可以在每次完成该按钮点击之后手动删除）汇总成为一个最终的结果文件——csv格式。后者按钮则是下载用户选定的任务所生成的csv文件，单个任务被选择即单文件csv，多任务被选择自动压缩为压缩包后下载
    其余的页面基本上都是查看，不再过多说明，
    
    


需要知道作为服务端的电脑()在局域网中所分配的IP，这个ip将会是其余机器访问的IP


这个工具分为两部分
   
    后台——也就是这个小工程
    
   
    单机请求数据文件——也就是文件夹“client”

创建一个此项目的虚拟环境并激活

    python -m venv venv

后台采用Django框架，使用的时候需要启动服务，
    
    首先是环境只需要做一次就好，
    安装所有的相关依赖(使用了清华镜像)
    pip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple
    
    然后做数据迁移，由于不确定有无mysql，所以采用了Django自带的sqlite3数据库
    
    生成迁移(work/migrations文件夹中除了__init__.py之外还有其余迁移文件，请删除其余迁移文件)
    python manage.py makemigrations
    
    将迁移同步至数据库
    python manage.py migrate
    创建一个超级用户用来登录管理后台
    python manage.py createsuperuser
    输入用户名，邮箱以及密码，随意填写一个记得住的就好

启动服务
    
    python manage.py runserver 0.0.0.0:8000
    


##启动服务
上述的步骤完成之后，查看程序是否正常运行，如果是，则可以开始各个IP的申请、运算、返回数据流程


##IP上传数据文件和对应的函数文件
新一轮的需求之后，我完全抛弃了之前的做法，现在后台中的所有任务都是其他IP上传的，使得服务端成为纯粹的服务端，各IP作为数据上传者身份上传数据文件和对应计算该数据的函数文件，IP在上传之前需要运行sign.py文件注册一个数据上传者身份，函数文件上传至functions文件夹下，之下的一级文件夹名字对应的是该IP在数据库的ID，为整数，下级即为该IP所有时间段上传的所有函数文件，不同时段上传的同名函数文件会以之后上传的为主（后来的文件覆盖之前的同名文件），需要注意函数文件需要先上传，数据文件上传至data文件夹中，会自动放置在当日日期文件夹下，两种文件在上传后都会自动生成数据库中相应的数据集，无须手动操作，


##各IP同步服务端函数文件
完成之后就可以开始工作，所有的IP都可以通过运行file_synchronization.py文件来同步函数文件，


##各IP开始申请数据集运算
各IP运行predict.py文件开始循环申请数据集，可能会有失败的情况，再次运行即可。



        
##用户端使用手册
    
    工作逻辑——
        所有IP拥有数据提供者和运算者两种身份，第一种身份上传用户希望运算的数据文件和对应执行该运算的函数文件，第二种身份申请数据集运算并返回运算值
        1.数据提供者
            数据提供者将函数文件放置于upload_functions文件夹中，不能存在文件夹，必须是所有文件在该目录下，命名规则为——”用户自定的该函数希望运算IP的能力等级“+”-“+”函数的名字“+".py",比如一个电力大数据运算函数文件，希望运算的IP能力等级在5级（1-5，越高能力越强），函数名为electric，函数文件名字为”5-electric.py“
            完成之后运行upload_func.py文件
            数据提供者将数据文件放置于upload_data文件夹中，文件名为对应的函数名字，”electric.pkl“,电力大数据和大整数分解目前不支持文件夹嵌套，只能是把文件全部放在该目录下，文本处理任务支持文件嵌套，但是文件夹名字必须为对应函数名字，比如测试的金庸和古龙小说，应该全部放在NLP文件夹下，txt文件可以出现在NLP下的每一级文件夹中
            完成之后运行upload_data.py文件
        2.运算者
            运算者需要先同步所有存储在后台中的函数文件，对的，目前是所有，因为后台中只根据用户ID做文件夹区分，没有日期的概念，我觉得没啥必要，所以就全下载了
            运行文件file_synchronization.py完成函数文件同步
            现在可以开始申请数据集了
            运行文件predict.py，开始持续申请数据集，若中断请重复运行此文件，还是不行的话进入dajngo管理后台，进入菜单——分级处理——用户，将该用户对应status——”空闲“点击为True状态后点击上方保存按钮，
            

## 函数文件上传至lzudatafatory中去     
##上传的函数文件函数代码格式请按照upload_function文件夹中示例
##上传的数据文件只有文字处理可以有文件夹嵌套
##上传流程中函数可以反复上传，但是数据文件只能上传一次，实际上上传之后所有的文件就被打包到upload_data_backup文件夹中