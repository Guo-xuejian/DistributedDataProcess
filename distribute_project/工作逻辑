            这个工具的工作逻辑
还是那句话，不给钱的外包，由于单机处理一个较大的数据文件需要的时间太长，据詹老师给我说，他处理一个两千万级别的文件，服务器单机需要7个小时，所以使用者才会想到多机处理，
超算中心再牛也只有一个，还不一定有资格用，那么大数据处理就太耗时了，詹老师又是老师，还有机房的使用资格，
只要我的机器够多，超算中心也是弟弟，单机的效率由性能和算法决定，咱们算法不算好，那就把数据分发出去，多机处理，别样的并发计算数据
那逻辑就出来了，把一个数据文件拆分为若干个数据集，每一台局域网内的机器（IP）可以申请得到一个数据集，通过写好的函数计算好之后生成一个文件，返回至后端
当一个任务的所有数据集都上传之后，就可以聚合为一个最终的结果文件供下载，同时后台记录下了IP的申请信息和上交信息，使用者可以回溯查询相关的信息
使用者想要干什么？
    1.上传一些文件
    2.选定哪些任务现在做
    3.把该任务对应的数据文件拆分为若干个数据集，实际上并没有生成若干文件，只是在数据库中增加了若干条记录，
    每一条记录中包含数据集的唯一名字、所属的任务等信息，它的名字中后缀是数字，该数字实际上就是在原完整数据文件中的排序，举个例子，data任务有700000条数据需要计算
    我们设定该任务的分发数据集中包含8000条数据，也就是models.py中的Task.num=8000，那么就应该有700000/8000个数据集，向上取整，也就是88个数据集，每一个数据集名字的后缀从1--88，
    比如	data-05-09-16*1这个数据集，中间的是五月九日16点的意思，因为需要保存每个任务的信息很久，所有为了避免出现同名（因为这个字段是主键），加上时间，后缀1也就意味着该数据集在对应的data任务中编号5
    它所对应的是原数据文件中对应的就是第0--7999条数据，总计8000，
    4.每一个IP可以开始申请数据集了，当它们通过web请求申请数据集时，要付加上自己的IP，后台根据IP自动注册该IP为用户，用户有一个属性status，为True时该用户可以申请，为False时意味着
    该IP正处在下载和运算上传阶段，上传运算结果之后status回到True